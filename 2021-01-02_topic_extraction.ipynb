{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic extraction\n",
    "\n",
    "1. Write several texts\n",
    "2. Extract non-rubbish words\n",
    "3. Apply NMF to get some components\n",
    "4. Do reconstructions of the texts => how well does this return the original texts?\n",
    "4. Try clustering on the words => can I extract topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write texts\n",
    "\n",
    "text_1 = \"The sun is shining outside, it's a nice day.\"\n",
    "text_2 = \"There may be showers outside, in which case the day would not be nice.\"\n",
    "text_3 = \"Sun, showers and wind are all very common in the UK on any particular day.\"\n",
    "\n",
    "text_4 = \"Machine learning is an interesting topic to study.\"\n",
    "text_5 = \"Studying is a good thing to do, whether it's for machine learning or some other topic.\"\n",
    "text_6 = \"Good students can master machine learning quickly and move on to other exciting topics.\"\n",
    "\n",
    "# a list of text pieces\n",
    "texts = [text_1, text_2, text_3, text_4, text_5, text_6]\n",
    "\n",
    "# one large piece of text\n",
    "# all_text = \"\"\n",
    "# for text in texts:\n",
    "#     all_text += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 24\n",
      "Vocabulary content: {'sun': 18, 'shining': 13, 'outside': 10, 'nice': 9, 'day': 2, 'showers': 14, 'case': 0, 'wind': 23, 'common': 1, 'uk': 22, 'particular': 11, 'machine': 7, 'learning': 6, 'interesting': 5, 'topic': 20, 'study': 16, 'studying': 17, 'good': 4, 'thing': 19, 'students': 15, 'master': 8, 'quickly': 12, 'exciting': 3, 'topics': 21}\n"
     ]
    }
   ],
   "source": [
    "# extract non-rubbish words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "vect.fit(texts)\n",
    "\n",
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"Vocabulary content: {}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "# !conda install nltk spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "# !conda install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wordbags: <6x24 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 36 stored elements in Compressed Sparse Row format>\n",
      "Dense representation of wordbags:\n",
      "[[0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1]\n",
      " [0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0]\n",
      " [0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# one-hot-encode non-rubbish words\n",
    "\n",
    "wordbags = vect.transform(texts)\n",
    "print(\"Wordbags: {}\".format(repr(wordbags)))\n",
    "print(\"Dense representation of wordbags:\\n{}\".format(wordbags.toarray()))\n",
    "\n",
    "# could normalise, so sum of squares (length of vector) per text is constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nmf.components_\n",
      " [[0.         0.         0.         0.36664321 0.67797954 0.24265309\n",
      "  0.92063263 0.92063263 0.36664321 0.         0.         0.\n",
      "  0.36664321 0.         0.         0.36664321 0.24265309 0.31133632\n",
      "  0.         0.31133632 0.55398942 0.36664321 0.         0.        ]\n",
      " [0.30189078 0.35808492 0.96186648 0.         0.         0.\n",
      "  0.         0.         0.         0.60378155 0.60378155 0.35808492\n",
      "  0.         0.30189078 0.6599757  0.         0.         0.\n",
      "  0.6599757  0.         0.         0.         0.35808492 0.35808492]] \n",
      "\n",
      "\n",
      "wordbags_nmf\n",
      " [[0.         0.97227023]\n",
      " [0.         0.97227023]\n",
      " [0.         1.15324925]\n",
      " [0.83604845 0.        ]\n",
      " [1.0726929  0.        ]\n",
      " [1.26324987 0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bkotryna/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1076: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    }
   ],
   "source": [
    "# apply NMF to get some components - eg 2 components\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=2, random_state=0)\n",
    "# fit NMF model to our joint large piece of text\n",
    "nmf.fit(wordbags)\n",
    "# show the obtained components\n",
    "print(\"nmf.components_\\n\", nmf.components_, \"\\n\\n\")\n",
    "\n",
    "# transform texts\n",
    "wordbags_nmf = nmf.transform(wordbags)\n",
    "# show transformed data\n",
    "print(\"wordbags_nmf\\n\", wordbags_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.930703308172537"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordbags_nmf[2][1] / wordbags_nmf[0][1] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wordbags_nmf[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordbags:\n",
      " [[0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1]\n",
      " [0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0]\n",
      " [0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0]] \n",
      "\n",
      "reconstruction:\n",
      " [[0.29351941 0.34815531 0.93519414 0.         0.         0.\n",
      "  0.         0.         0.         0.58703883 0.58703883 0.34815531\n",
      "  0.         0.29351941 0.64167473 0.         0.         0.\n",
      "  0.64167473 0.         0.         0.         0.34815531 0.34815531]\n",
      " [0.29351941 0.34815531 0.93519414 0.         0.         0.\n",
      "  0.         0.         0.         0.58703883 0.58703883 0.34815531\n",
      "  0.         0.29351941 0.64167473 0.         0.         0.\n",
      "  0.64167473 0.         0.         0.         0.34815531 0.34815531]\n",
      " [0.34815531 0.41296117 1.1092718  0.         0.         0.\n",
      "  0.         0.         0.         0.69631062 0.69631062 0.41296117\n",
      "  0.         0.34815531 0.76111648 0.         0.         0.\n",
      "  0.76111648 0.         0.         0.         0.41296117 0.41296117]\n",
      " [0.         0.         0.         0.30653149 0.56682374 0.20286974\n",
      "  0.76969348 0.76969348 0.30653149 0.         0.         0.\n",
      "  0.30653149 0.         0.         0.30653149 0.20286974 0.26029225\n",
      "  0.         0.26029225 0.46316199 0.30653149 0.         0.        ]\n",
      " [0.         0.         0.         0.39329557 0.72726384 0.26029225\n",
      "  0.98755609 0.98755609 0.39329557 0.         0.         0.\n",
      "  0.39329557 0.         0.         0.39329557 0.26029225 0.33396827\n",
      "  0.         0.33396827 0.59426052 0.39329557 0.         0.        ]\n",
      " [0.         0.         0.         0.46316199 0.85645757 0.30653149\n",
      "  1.16298905 1.16298905 0.46316199 0.         0.         0.\n",
      "  0.46316199 0.         0.         0.46316199 0.30653149 0.39329557\n",
      "  0.         0.39329557 0.69982706 0.46316199 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# reconstruct texts from NMF components - how well does this work?\n",
    "\n",
    "reconstruction = nmf.inverse_transform(wordbags_nmf)\n",
    "print (\"wordbags:\\n\", wordbags.toarray(), '\\n\\nreconstruction:\\n', reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wordbags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# for AgglomerativeClustering: it complained that data was sparse\n",
    "y = wordbags.todense()\n",
    "\n",
    "# dense => bmp\n",
    "# sparse => just say posiitons for 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1]\n",
      " [0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0]\n",
      " [0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.matrix"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y)\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Agg\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "n_clusters = 2\n",
    "agg = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "\n",
    "assignement = agg.fit_predict(y)\n",
    "display(assignement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try clustering on the words - can I extract topics?\n",
    "# BTW it's fun to play with the number of clusters - eg go from 2 to 3 or 4\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "n_clusters = 2\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "\n",
    "assignement = kmeans.fit_predict(wordbags)\n",
    "display(assignement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.98142397, 1.33333333],\n",
       "       [2.98142397, 1.33333333],\n",
       "       [3.29983165, 1.76383421],\n",
       "       [1.49071198, 2.90593263],\n",
       "       [1.37436854, 3.07318149],\n",
       "       [1.79505494, 3.38296386]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distances from each cluster centre\n",
    "\n",
    "kmeans.transform(wordbags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-14.000000000000005"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster number: 1\n",
      "[0 0 0 0 0 0]\n",
      "-25.0 \n",
      "\n",
      "\n",
      "cluster number: 2\n",
      "[1 1 1 0 0 0]\n",
      "-14.000000000000005 \n",
      "\n",
      "\n",
      "cluster number: 3\n",
      "[0 0 0 2 2 1]\n",
      "-9.166666666666668 \n",
      "\n",
      "\n",
      "cluster number: 4\n",
      "[1 1 3 0 0 2]\n",
      "-4.5 \n",
      "\n",
      "\n",
      "cluster number: 5\n",
      "[1 1 3 2 4 0]\n",
      "-2.0 \n",
      "\n",
      "\n",
      "cluster number: 6\n",
      "[0 5 4 3 1 2]\n",
      "-0.0 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "numbers = [1,2,3,4,5,6]\n",
    "\n",
    "for n_clusters in numbers:\n",
    "    print(\"cluster number:\", n_clusters)\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    assignement = kmeans.fit_predict(wordbags)\n",
    "    print(assignement)\n",
    "    print(kmeans.score(wordbags), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.33333333, 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.66666667,\n",
       "        0.66666667, 0.33333333, 0.        , 0.33333333, 0.66666667,\n",
       "        0.        , 0.        , 0.        , 0.66666667, 0.        ,\n",
       "        0.        , 0.        , 0.33333333, 0.33333333],\n",
       "       [0.        , 0.        , 0.        , 0.33333333, 0.66666667,\n",
       "        0.33333333, 1.        , 1.        , 0.33333333, 0.        ,\n",
       "        0.        , 0.        , 0.33333333, 0.        , 0.        ,\n",
       "        0.33333333, 0.33333333, 0.33333333, 0.        , 0.33333333,\n",
       "        0.66666667, 0.33333333, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see what the distance is from each datapoint to their cluster centre\n",
    "\n",
    "# get cluster coordinates\n",
    "kmeans.cluster_centers_\n",
    "\n",
    "# then give up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in n_clusters:\n",
    "#     dataInCluster = wordbags[assignement[cluster==i].rowNames,]\n",
    "#     distance = norm(dataInCluster-clusterCenter[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
